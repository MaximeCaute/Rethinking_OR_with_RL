{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Proof of Concept on Simple Examples\n",
    "\n",
    "The purpose of this notebook is to test our learning ability on simple, non-stochastic policies, for validation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here consider a simple policy with Nearest Neighbor Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch, for NNs\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Time, for time perf. measurements\n",
    "import time\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "#importing functions for time evaluation\n",
    "from evaluate_times import generate_event, generate_times, print_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 30\n",
    "\n",
    "REQUESTS_PER_TABLE = 100\n",
    "\n",
    "FLEET_SIZE = 30\n",
    "VEHICLES_CAPACITY = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data\n",
    "\n",
    "For simplicity purpose, at first our data will be created out of thin air for the requests.\n",
    "\n",
    "We will keep a 30 \\* 30 pixels image, where requests will be asked from two random points. Vehicles will be randomly positioned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixel(image_size = 30):\n",
    "    x,y = np.random.randint(low= 0, high = image_size, size = (2,))\n",
    "    return [x,y]\n",
    "\n",
    "def generate_vehicles_info(image_size = 30, fleet_size = 30, vehicles_capacity = -1):\n",
    "    \"\"\"\n",
    "        - consider_capacity: int or int list. The capacity of the vehicles. \n",
    "            If it is an integer, the fleet is homogenous with respect to capacity, and have this value.\n",
    "            If a value is -1, the vehicle has infinite capacity\n",
    "    \"\"\"\n",
    "    vehicles_info = []\n",
    "    \n",
    "    # Setting capacities properly\n",
    "    if isinstance(vehicles_capacity, int):\n",
    "        value = vehicles_capacity\n",
    "        vehicles_capacity = [value for i in range(fleet_size)]\n",
    "    \n",
    "    \n",
    "    for k in range(fleet_size):\n",
    "        vehicle_position = random_pixel(image_size)\n",
    "        vehicles_destination = random_pixel(image_size)\n",
    "        vehicle_capacity = vehicles_capacity[k]\n",
    "        vehicle_load = np.random.randint(low = 0,high=vehicle_capacity)\n",
    "        \n",
    "        vehicle_info = [vehicle_capacity, vehicle_load]+ vehicle_position+ vehicles_destination\n",
    "        \n",
    "        vehicles_info.append(vehicle_info)\n",
    "    return vehicles_info\n",
    "\n",
    "def generate_data_table(request_amount,image_size = 30, fleet_size=30, vehicles_capacity =-1):\n",
    "    data = {}\n",
    "    for n in range(request_amount):\n",
    "\n",
    "        request = []\n",
    "\n",
    "        pickup_location = random_pixel(image_size)\n",
    "        dropoff_location = random_pixel(image_size)\n",
    "\n",
    "        request_info = pickup_location + dropoff_location\n",
    "        vehicles_info = generate_vehicles_info(image_size, fleet_size, vehicles_capacity)\n",
    "\n",
    "        request.append(request_info)\n",
    "\n",
    "        data[n] = request+vehicles_info\n",
    "    return data\n",
    "\n",
    "data_tables = []\n",
    "for i in range(90):\n",
    "    data_tables.append(generate_data_table(REQUESTS_PER_TABLE,IMAGE_SIZE, FLEET_SIZE, VEHICLES_CAPACITY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 14, 21, 2], [6, 2, 15, 7, 7, 4], [6, 1, 9, 29, 4, 20], [6, 3, 29, 7, 23, 15], [6, 5, 28, 9, 28, 14], [6, 5, 29, 10, 26, 5], [6, 2, 9, 25, 7, 25], [6, 1, 22, 21, 2, 29], [6, 5, 1, 10, 14, 12], [6, 0, 23, 7, 26, 9], [6, 4, 19, 12, 3, 25], [6, 0, 16, 20, 16, 12], [6, 5, 15, 4, 25, 19], [6, 2, 4, 17, 29, 24], [6, 4, 19, 6, 29, 7], [6, 4, 11, 29, 13, 4], [6, 4, 19, 20, 0, 8], [6, 1, 12, 10, 9, 26], [6, 1, 24, 6, 16, 8], [6, 3, 13, 3, 28, 7], [6, 1, 18, 7, 15, 26], [6, 3, 28, 3, 26, 3], [6, 5, 17, 21, 14, 6], [6, 2, 22, 12, 17, 12], [6, 0, 24, 0, 6, 29], [6, 5, 20, 17, 24, 14], [6, 4, 0, 5, 28, 5], [6, 2, 0, 24, 4, 19], [6, 3, 22, 22, 10, 28], [6, 1, 26, 17, 17, 24], [6, 4, 11, 25, 15, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(data_tables[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy definition\n",
    "\n",
    "We now define our policy, which basically implements nearest neighbour search (without capacity constraints, at first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_policy(data):\n",
    "    request_info = data[0]\n",
    "    pickup_location = request_info[0:2]\n",
    "    \n",
    "    vehicles_request_infos = data[1:-1]\n",
    "    \n",
    "    closest_dist = 10000\n",
    "    selected_vehicle = -1\n",
    "    for i,vehicle_request_infos in enumerate(vehicles_request_infos):\n",
    "        vehicle_position = vehicle_request_infos[2:4]\n",
    "        \n",
    "        vehicle_distance = np.sqrt(\n",
    "                    (vehicle_position[0]-pickup_location[0])**2 +\n",
    "                    (vehicle_position[1]-pickup_location[1])**2 \n",
    "        )\n",
    "        \n",
    "        if vehicle_distance < closest_dist:\n",
    "            selected_vehicle = i+1\n",
    "            closest_dist = vehicle_distance\n",
    "    \n",
    "    return selected_vehicle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a much simpler policy: always calling vehicle 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_car_policy(data):\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus generate the solutions from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solutions(data, policy):\n",
    "    solutions = {}\n",
    "    for k in data.keys():\n",
    "\n",
    "        selected_vehicle = policy(data[k])\n",
    "\n",
    "        solutions[k] = selected_vehicle\n",
    "    return solutions\n",
    "\n",
    "solutions_tables = []\n",
    "for data in data_tables:\n",
    "    solutions_tables.append(generate_solutions(data, single_car_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3, 1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3, 11: 3, 12: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 3, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3, 24: 3, 25: 3, 26: 3, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 3, 33: 3, 34: 3, 35: 3, 36: 3, 37: 3, 38: 3, 39: 3, 40: 3, 41: 3, 42: 3, 43: 3, 44: 3, 45: 3, 46: 3, 47: 3, 48: 3, 49: 3, 50: 3, 51: 3, 52: 3, 53: 3, 54: 3, 55: 3, 56: 3, 57: 3, 58: 3, 59: 3, 60: 3, 61: 3, 62: 3, 63: 3, 64: 3, 65: 3, 66: 3, 67: 3, 68: 3, 69: 3, 70: 3, 71: 3, 72: 3, 73: 3, 74: 3, 75: 3, 76: 3, 77: 3, 78: 3, 79: 3, 80: 3, 81: 3, 82: 3, 83: 3, 84: 3, 85: 3, 86: 3, 87: 3, 88: 3, 89: 3, 90: 3, 91: 3, 92: 3, 93: 3, 94: 3, 95: 3, 96: 3, 97: 3, 98: 3, 99: 3}\n"
     ]
    }
   ],
   "source": [
    "print(solutions_tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation\n",
    "\n",
    "We now define the neural network we will use for learning. Note that this is the one previously used for non-recurrent inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final2(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        #x7=x6\n",
    "        \n",
    "        #x8 = self.fc3(x7)\n",
    "        #x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_tensors(batch_indices, vehicles_amount, image_size, \n",
    "                       data, data_y):\n",
    "    \"\"\"\n",
    "    Instantiate training data for later filling and tensor conversion.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_indices)\n",
    "    dist = 1 #/ (image_size - 1)\n",
    "    \n",
    "    main_input = np.zeros((batch_size, vehicles_amount+1, image_size, image_size))\n",
    "    main_output = []\n",
    "    # 2D image\n",
    "    aux_input = []\n",
    "    aux_output = np.zeros((batch_size, 2))\n",
    "    \n",
    "    \n",
    "    for k, cl in enumerate(batch_indices):\n",
    "        \n",
    "        # Getting input for request\n",
    "        situation_data = data[cl]\n",
    "        \n",
    "        # Filling request-related information in main input\n",
    "        request_data = situation_data[0]\n",
    "        pickup_xpos, pickup_ypos = int(situation_data[0][0] // dist), int(situation_data[0][1] // dist)\n",
    "        dropoff_xpos, dropoff_ypos = int(situation_data[0][2] // dist),  int(situation_data[0][3] // dist)\n",
    "        main_input[k][0][pickup_xpos][pickup_ypos] = 1\n",
    "        main_input[k][0][dropoff_xpos][dropoff_ypos] = -1\n",
    "        \n",
    "        # Filling vehicle positions in main input\n",
    "        for vehicle_index in range(1, 31):\n",
    "            vehicle_data = situation_data[vehicle_index]\n",
    "            vehicle_xpos = int(vehicle_data[2] // dist)\n",
    "            vehicle_ypos = int(vehicle_data[3] // dist)\n",
    "            \n",
    "            main_input[k][vehicle_index][vehicle_xpos][vehicle_ypos] = 1\n",
    "        \n",
    "        # Filling all vehicles information in auxiliary input\n",
    "        total_vehicles_data = situation_data[1:]\n",
    "        aux_input.append(torch.tensor(np.asarray(total_vehicles_data)).type(torch.FloatTensor))\n",
    "        \n",
    "        \n",
    "        # Getting output for request\n",
    "        assigned_vehicle = int(data_y[cl]) + 1\n",
    "        \n",
    "        # As no treatment is involved, main output is filled out of the loop.\n",
    "        \n",
    "        # Filling assigned vehicle position in auxiliary output\n",
    "        assigned_vehicle_xpos = situation_data[assigned_vehicle][2]#loc = int(data_y[cl])\n",
    "                                                                #loc_y[k] = [data[cl][loc + 1][2], data[cl][loc + 1][3]]\n",
    "        assigned_vehicle_ypos = situation_data[assigned_vehicle][3]\n",
    "        aux_output[k] = [assigned_vehicle_xpos, assigned_vehicle_ypos]\n",
    "\n",
    "    # Filling assigned vehicles - (almost no treatment involved)\n",
    "    output_list = [data_y[i] for i in batch_indices]\n",
    "    main_output = np.hstack(output_list)\n",
    "    \n",
    "    # Turning inputs to tensors\n",
    "    main_input_tensor = torch.tensor(main_input).type(torch.FloatTensor)\n",
    "    aux_input_tensor = torch.stack(aux_input).type(torch.FloatTensor)\n",
    "    main_output_tensor = torch.tensor(main_output).type(torch.LongTensor)\n",
    "    aux_output_tensor = torch.tensor(aux_output).type(torch.FloatTensor)\n",
    "\n",
    "    return main_input_tensor, aux_input_tensor, main_output_tensor, aux_output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                e, im_size, weighted, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Epochs for recurrent, double-input double-output not clipped nor expanded networks.\n",
    "    \"\"\"\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1 / (im_size - 1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "                    \n",
    "            train_x, train_x_aux, train_y, train_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "            \n",
    "            #  set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "\n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "            # take into account difference in order of magnitude\n",
    "            #batch_loss = 100*weighted * criterion1(output1, train_y_aux) + (1 - weighted) * criterion2(output2,\n",
    "            #                                                                                       train_y)\n",
    "            batch_loss = weighted * criterion1(aux_output, train_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                           train_y)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            a = chosen_vehicle_indices\n",
    "            acc.append(float((train_y == a).sum()) / len(train_y))\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "\n",
    "    # model.eval()\n",
    "    for k, TABLE in enumerate(test_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "\n",
    "        idx = list(data.keys())\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "\n",
    "            test_x, test_x_aux, test_y, test_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "            \n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "                \n",
    "            batch_loss = weighted * criterion1(aux_output, test_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                          test_y)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            a = chosen_vehicle_indices\n",
    "\n",
    "            test_acc.append(float((test_y == a).sum()) / len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "\n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e + 1, sum_loss,\n",
    "                                                                                                      np.mean(acc),\n",
    "                                                                                                      test_loss,\n",
    "                                                                                                      np.mean(\n",
    "                                                                                                          test_acc)),\n",
    "          end=\"\")\n",
    "    return sum_loss, np.sum(acc) / len(acc), test_loss, np.sum(test_acc) / len(test_acc), idx_failures\n",
    "\n",
    "\n",
    "def evaluating_model(train_tables, test_tables, model, im_size, n_epochs,\n",
    "                     simple=False, weighted=0.5, lr = 0.0001, minibatch_size= 50,\n",
    "                    ):\n",
    "                    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss()\n",
    "    criterion1 = nn.MSELoss()\n",
    "\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                        epoch, im_size, weighted, minibatch_size)\n",
    "\n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2 * np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc == max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "\n",
    "    return loss, acc, test_loss, test_acc, idx_f, times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE 1. Double Input Double Output - Complete Final Net. IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 306.891 Accuracy: 0.497 Test Loss: 155.835 Accuracy: 0.480\n",
      "Average time per epoch 7.787s +- 2.082\n",
      "Max accuracy of 0.480 achieved at epoch 17\n",
      "Epoch 20. Train Loss: 308.090 Accuracy: 0.479 Test Loss: 157.390 Accuracy: 0.480\n",
      "Average time per epoch 8.299s +- 1.390\n",
      "Max accuracy of 0.480 achieved at epoch 18\n",
      "Epoch 20. Train Loss: 309.381 Accuracy: 0.474 Test Loss: 156.006 Accuracy: 0.460\n",
      "Average time per epoch 7.956s +- 1.088\n",
      "Max accuracy of 0.460 achieved at epoch 13\n",
      "Epoch 20. Train Loss: 306.584 Accuracy: 0.490 Test Loss: 153.885 Accuracy: 0.540\n",
      "Average time per epoch 8.835s +- 1.119\n",
      "Max accuracy of 0.540 achieved at epoch 17\n",
      "Average accuracy 0.490 +- 0.030. Av loss 155.779\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n",
      "\n",
      "CASE 2. Double Input Double Output - Complete Final Net. Learning rate = 0.001 IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 142.988 Accuracy: 0.721 Test Loss: 71.944 Accuracy: 0.7200\n",
      "Average time per epoch 8.073s +- 1.768\n",
      "Max accuracy of 0.720 achieved at epoch 10\n",
      "Epoch 20. Train Loss: 142.027 Accuracy: 0.729 Test Loss: 70.216 Accuracy: 0.7400\n",
      "Average time per epoch 7.840s +- 1.225\n",
      "Max accuracy of 0.740 achieved at epoch 10\n",
      "Epoch 20. Train Loss: 142.391 Accuracy: 0.724 Test Loss: 75.505 Accuracy: 0.6800\n",
      "Average time per epoch 7.180s +- 1.598\n",
      "Max accuracy of 0.680 achieved at epoch 7\n",
      "Epoch 20. Train Loss: 141.132 Accuracy: 0.733 Test Loss: 71.636 Accuracy: 0.7200\n",
      "Average time per epoch 7.993s +- 0.497\n",
      "Max accuracy of 0.720 achieved at epoch 9\n",
      "Average accuracy 0.715 +- 0.022. Av loss 72.325\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n",
      "\n",
      "CASE 3. Double Input Double Output - Reduced Final Net. IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 0.004 Accuracy: 1.000 Test Loss: 0.002 Accuracy: 1.000\n",
      "Average time per epoch 6.701s +- 1.248\n",
      "Max accuracy of 1.000 achieved at epoch 0\n",
      "Epoch 20. Train Loss: 0.002 Accuracy: 1.000 Test Loss: 0.001 Accuracy: 1.000\n",
      "Average time per epoch 6.828s +- 1.216\n",
      "Max accuracy of 1.000 achieved at epoch 0\n",
      "Epoch 20. Train Loss: 0.003 Accuracy: 1.000 Test Loss: 0.001 Accuracy: 1.000\n",
      "Average time per epoch 6.856s +- 0.581\n",
      "Max accuracy of 1.000 achieved at epoch 0\n",
      "Epoch 20. Train Loss: 0.001 Accuracy: 1.000 Test Loss: 0.000 Accuracy: 1.000\n",
      "Average time per epoch 6.536s +- 1.281\n",
      "Max accuracy of 1.000 achieved at epoch 0\n",
      "Average accuracy 1.000 +- 0.000. Av loss 0.001\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n"
     ]
    }
   ],
   "source": [
    "print('\\nCASE 1. Double Input Double Output - Complete Final Net. IMAGE SIZE = 30\\n')\n",
    "total_acc, total_loss = [], []\n",
    "im_size = 30\n",
    "nv = 30\n",
    "kernel_size=3\n",
    "n_epochs = 20\n",
    "\n",
    "np.random.seed(4914)\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    tables = list(range(1, 90))\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model1= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model1, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "    \n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model1.parameters()), sum(\n",
    "        p.numel() for p in model1.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 2. Double Input Double Output - Complete Final Net. Learning rate = 0.001 IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model2= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model2, IMAGE_SIZE, n_epochs, lr = 0.001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model2.parameters()), sum(\n",
    "        p.numel() for p in model2.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 3. Double Input Double Output - Reduced Final Net. IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model3= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model3, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model3.parameters()), sum(\n",
    "        p.numel() for p in model3.parameters() if p.requires_grad)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest neighbour: 6\n",
      "Result comparison: main output\n",
      "Network: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "Policy: tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "\n",
      "Result comparison: auxiliary output\n",
      "Network: tensor([[-1.1236e-01,  1.6082e+00],\n",
      "        [-2.9400e-02,  1.6948e+00],\n",
      "        [ 2.0260e-01,  1.6590e+00],\n",
      "        [-9.2893e-02,  1.6713e+00],\n",
      "        [ 3.4635e-02,  1.6135e+00],\n",
      "        [-1.3657e-01,  1.5894e+00],\n",
      "        [ 6.4408e-02,  1.6515e+00],\n",
      "        [ 3.7017e-02,  1.6779e+00],\n",
      "        [ 1.3262e-02,  1.7290e+00],\n",
      "        [-3.7239e-04,  1.7213e+00],\n",
      "        [ 8.1458e-02,  1.5639e+00],\n",
      "        [-8.2075e-03,  1.6255e+00]], grad_fn=<AddmmBackward>)\n",
      "Policy: tensor([[18., 20.],\n",
      "        [14., 14.],\n",
      "        [ 1., 20.],\n",
      "        [25., 18.],\n",
      "        [10., 25.],\n",
      "        [28.,  9.],\n",
      "        [ 0.,  1.],\n",
      "        [22., 16.],\n",
      "        [ 3., 24.],\n",
      "        [27.,  7.],\n",
      "        [ 9., 15.],\n",
      "        [ 5.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "data = data_tables[0]\n",
    "solutions= solutions_tables[0]\n",
    "\n",
    "x,x_aux,y, y_aux = get_network_tensors([i for i in range(12)], FLEET_SIZE, IMAGE_SIZE, data, solutions)\n",
    "situation_data = x[0]\n",
    "sd = situation_data\n",
    "\n",
    "nnb= nearest_neighbour_policy(data[1])\n",
    "print(\"\")\n",
    "print(\"Nearest neighbour:\",nnb)\n",
    "#model= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "out_aux,out = model2(x, x_aux)\n",
    "_, out_i = torch.max(out, 1)\n",
    "print(\"Result comparison: main output\")\n",
    "print(\"Network:\",out_i)\n",
    "print(\"Policy:\",y)\n",
    "print(\"\")\n",
    "print(\"Result comparison: auxiliary output\")\n",
    "print(\"Network:\",out_aux)\n",
    "print(\"Policy:\",y_aux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Linear model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
