{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Proof of Concept on Simple Examples : Nearest neighbour\n",
    "\n",
    "The purpose of this notebook is to test our learning ability on simple, non-stochastic policies, for validation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here consider a simple policy with Nearest Neighbor Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch, for NNs\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Time, for time perf. measurements\n",
    "import time\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "#importing functions for time evaluation\n",
    "from evaluate_times import generate_event, generate_times, print_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 30\n",
    "\n",
    "REQUESTS_PER_TABLE = 100\n",
    "\n",
    "FLEET_SIZE = 30\n",
    "VEHICLES_CAPACITY = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data\n",
    "\n",
    "For simplicity purpose, at first our data will be created out of thin air for the requests.\n",
    "\n",
    "We will keep a 30 \\* 30 pixels image, where requests will be asked from two random points. Vehicles will be randomly positioned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixel(image_size = 30):\n",
    "    x,y = np.random.randint(low= 0, high = image_size, size = (2,))\n",
    "    return [x,y]\n",
    "\n",
    "def generate_vehicles_info(image_size = 30, fleet_size = 30, vehicles_capacity = -1):\n",
    "    \"\"\"\n",
    "        - consider_capacity: int or int list. The capacity of the vehicles. \n",
    "            If it is an integer, the fleet is homogenous with respect to capacity, and have this value.\n",
    "            If a value is -1, the vehicle has infinite capacity\n",
    "    \"\"\"\n",
    "    vehicles_info = []\n",
    "    \n",
    "    # Setting capacities properly\n",
    "    if isinstance(vehicles_capacity, int):\n",
    "        value = vehicles_capacity\n",
    "        vehicles_capacity = [value for i in range(fleet_size)]\n",
    "    \n",
    "    \n",
    "    for k in range(fleet_size):\n",
    "        vehicle_position = random_pixel(image_size)\n",
    "        vehicles_destination = random_pixel(image_size)\n",
    "        vehicle_capacity = vehicles_capacity[k]\n",
    "        vehicle_load = np.random.randint(low = 0,high=vehicle_capacity)\n",
    "        \n",
    "        vehicle_info = [vehicle_capacity, vehicle_load]+ vehicle_position+ vehicles_destination\n",
    "        \n",
    "        vehicles_info.append(vehicle_info)\n",
    "    return vehicles_info\n",
    "\n",
    "def generate_data_table(request_amount,image_size = 30, fleet_size=30, vehicles_capacity =-1):\n",
    "    data = {}\n",
    "    for n in range(request_amount):\n",
    "\n",
    "        request = []\n",
    "\n",
    "        pickup_location = random_pixel(image_size)\n",
    "        dropoff_location = random_pixel(image_size)\n",
    "\n",
    "        request_info = pickup_location + dropoff_location\n",
    "        vehicles_info = generate_vehicles_info(image_size, fleet_size, vehicles_capacity)\n",
    "\n",
    "        request.append(request_info)\n",
    "\n",
    "        data[n] = request+vehicles_info\n",
    "    return data\n",
    "\n",
    "data_tables = []\n",
    "for i in range(90):\n",
    "    data_tables.append(generate_data_table(REQUESTS_PER_TABLE,IMAGE_SIZE, FLEET_SIZE, VEHICLES_CAPACITY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 10, 5, 0], [6, 4, 17, 15, 27, 6], [6, 3, 10, 27, 18, 10], [6, 3, 22, 5, 19, 5], [6, 4, 24, 3, 17, 12], [6, 3, 12, 1, 25, 16], [6, 2, 16, 14, 3, 25], [6, 1, 1, 7, 5, 25], [6, 2, 10, 0, 24, 27], [6, 5, 15, 5, 10, 29], [6, 0, 26, 25, 3, 5], [6, 4, 28, 29, 19, 21], [6, 3, 5, 17, 17, 4], [6, 1, 12, 3, 7, 16], [6, 0, 7, 16, 17, 20], [6, 0, 16, 2, 19, 12], [6, 3, 19, 16, 20, 19], [6, 5, 4, 23, 25, 19], [6, 4, 14, 21, 4, 5], [6, 0, 2, 12, 3, 10], [6, 1, 19, 18, 16, 15], [6, 1, 10, 26, 29, 18], [6, 2, 17, 11, 20, 28], [6, 2, 5, 22, 24, 11], [6, 3, 5, 7, 20, 18], [6, 3, 26, 9, 19, 25], [6, 4, 15, 14, 4, 17], [6, 5, 21, 19, 18, 24], [6, 0, 28, 27, 21, 18], [6, 2, 10, 13, 29, 11], [6, 0, 1, 10, 11, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(data_tables[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy definition\n",
    "\n",
    "We now define our policy, which basically implements nearest neighbour search (without capacity constraints, at first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_policy(data):\n",
    "    request_info = data[0]\n",
    "    pickup_location = request_info[0:2]\n",
    "    \n",
    "    vehicles_request_infos = data[1:-1]\n",
    "    \n",
    "    closest_dist = 10000\n",
    "    selected_vehicle = -1\n",
    "    for i,vehicle_request_infos in enumerate(vehicles_request_infos):\n",
    "        vehicle_position = vehicle_request_infos[2:4]\n",
    "        \n",
    "        vehicle_distance = np.sqrt(\n",
    "                    (vehicle_position[0]-pickup_location[0])**2 +\n",
    "                    (vehicle_position[1]-pickup_location[1])**2 \n",
    "        )\n",
    "        \n",
    "        if vehicle_distance < closest_dist:\n",
    "            selected_vehicle = i+1\n",
    "            closest_dist = vehicle_distance\n",
    "    \n",
    "    return selected_vehicle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a much simpler policy: always calling vehicle 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_car_policy(data):\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus generate the solutions from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solutions(data, policy):\n",
    "    solutions = {}\n",
    "    for k in data.keys():\n",
    "\n",
    "        selected_vehicle = policy(data[k])\n",
    "\n",
    "        solutions[k] = selected_vehicle\n",
    "    return solutions\n",
    "\n",
    "solutions_tables = []\n",
    "for data in data_tables:\n",
    "    solutions_tables.append(generate_solutions(data, nearest_neighbour_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 24, 1: 10, 2: 18, 3: 12, 4: 12, 5: 24, 6: 18, 7: 3, 8: 23, 9: 7, 10: 2, 11: 29, 12: 23, 13: 29, 14: 17, 15: 12, 16: 29, 17: 1, 18: 6, 19: 4, 20: 21, 21: 27, 22: 1, 23: 4, 24: 28, 25: 6, 26: 29, 27: 22, 28: 22, 29: 13, 30: 11, 31: 5, 32: 28, 33: 14, 34: 22, 35: 18, 36: 2, 37: 29, 38: 6, 39: 13, 40: 27, 41: 25, 42: 28, 43: 10, 44: 29, 45: 13, 46: 27, 47: 11, 48: 2, 49: 1, 50: 1, 51: 25, 52: 2, 53: 10, 54: 19, 55: 13, 56: 7, 57: 4, 58: 19, 59: 15, 60: 15, 61: 28, 62: 25, 63: 18, 64: 27, 65: 5, 66: 12, 67: 19, 68: 10, 69: 13, 70: 20, 71: 8, 72: 1, 73: 13, 74: 29, 75: 7, 76: 29, 77: 18, 78: 2, 79: 14, 80: 6, 81: 6, 82: 10, 83: 24, 84: 15, 85: 27, 86: 9, 87: 27, 88: 28, 89: 15, 90: 4, 91: 22, 92: 4, 93: 9, 94: 22, 95: 18, 96: 7, 97: 5, 98: 9, 99: 21}\n"
     ]
    }
   ],
   "source": [
    "print(solutions_tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation\n",
    "\n",
    "We now define the neural network we will use for learning. Note that this is the one previously used for non-recurrent inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final2(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        #x7=x6\n",
    "        \n",
    "        #x8 = self.fc3(x7)\n",
    "        #x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_tensors(batch_indices, vehicles_amount, image_size, \n",
    "                       data, data_y):\n",
    "    \"\"\"\n",
    "    Instantiate training data for later filling and tensor conversion.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_indices)\n",
    "    dist = 1 #/ (image_size - 1)\n",
    "    \n",
    "    main_input = np.zeros((batch_size, vehicles_amount+1, image_size, image_size))\n",
    "    main_output = []\n",
    "    # 2D image\n",
    "    aux_input = []\n",
    "    aux_output = np.zeros((batch_size, 2))\n",
    "    \n",
    "    \n",
    "    for k, cl in enumerate(batch_indices):\n",
    "        \n",
    "        # Getting input for request\n",
    "        situation_data = data[cl]\n",
    "        \n",
    "        # Filling request-related information in main input\n",
    "        request_data = situation_data[0]\n",
    "        pickup_xpos, pickup_ypos = int(situation_data[0][0] // dist), int(situation_data[0][1] // dist)\n",
    "        dropoff_xpos, dropoff_ypos = int(situation_data[0][2] // dist),  int(situation_data[0][3] // dist)\n",
    "        main_input[k][0][pickup_xpos][pickup_ypos] = 1\n",
    "        main_input[k][0][dropoff_xpos][dropoff_ypos] = -1\n",
    "        \n",
    "        # Filling vehicle positions in main input\n",
    "        for vehicle_index in range(1, 31):\n",
    "            vehicle_data = situation_data[vehicle_index]\n",
    "            vehicle_xpos = int(vehicle_data[2] // dist)\n",
    "            vehicle_ypos = int(vehicle_data[3] // dist)\n",
    "            \n",
    "            main_input[k][vehicle_index][vehicle_xpos][vehicle_ypos] = 1\n",
    "        \n",
    "        # Filling all vehicles information in auxiliary input\n",
    "        total_vehicles_data = situation_data[1:]\n",
    "        aux_input.append(torch.tensor(np.asarray(total_vehicles_data)).type(torch.FloatTensor))\n",
    "        \n",
    "        \n",
    "        # Getting output for request\n",
    "        assigned_vehicle = int(data_y[cl]) + 1\n",
    "        \n",
    "        # As no treatment is involved, main output is filled out of the loop.\n",
    "        \n",
    "        # Filling assigned vehicle position in auxiliary output\n",
    "        assigned_vehicle_xpos = situation_data[assigned_vehicle][2]#loc = int(data_y[cl])\n",
    "                                                                #loc_y[k] = [data[cl][loc + 1][2], data[cl][loc + 1][3]]\n",
    "        assigned_vehicle_ypos = situation_data[assigned_vehicle][3]\n",
    "        aux_output[k] = [assigned_vehicle_xpos, assigned_vehicle_ypos]\n",
    "\n",
    "    # Filling assigned vehicles - (almost no treatment involved)\n",
    "    output_list = [data_y[i] for i in batch_indices]\n",
    "    main_output = np.hstack(output_list)\n",
    "    \n",
    "    # Turning inputs to tensors\n",
    "    main_input_tensor = torch.tensor(main_input).type(torch.FloatTensor)\n",
    "    aux_input_tensor = torch.stack(aux_input).type(torch.FloatTensor)\n",
    "    main_output_tensor = torch.tensor(main_output).type(torch.LongTensor)\n",
    "    aux_output_tensor = torch.tensor(aux_output).type(torch.FloatTensor)\n",
    "\n",
    "    return main_input_tensor, aux_input_tensor, main_output_tensor, aux_output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                e, im_size, weighted, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Epochs for recurrent, double-input double-output not clipped nor expanded networks.\n",
    "    \"\"\"\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1 #/ (im_size - 1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "                    \n",
    "            train_x, train_x_aux, train_y, train_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "            \n",
    "            #  set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "\n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "            # take into account difference in order of magnitude\n",
    "            #batch_loss = 100*weighted * criterion1(output1, train_y_aux) + (1 - weighted) * criterion2(output2,\n",
    "            #                                                                                       train_y)\n",
    "            batch_loss = weighted * criterion1(aux_output, train_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                           train_y)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            a = chosen_vehicle_indices\n",
    "            acc.append(float((train_y == a).sum()) / len(train_y))\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "\n",
    "    # model.eval()\n",
    "    for k, TABLE in enumerate(test_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "\n",
    "        idx = list(data.keys())\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "\n",
    "            test_x, test_x_aux, test_y, test_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "            \n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "                \n",
    "            batch_loss = weighted * criterion1(aux_output, test_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                          test_y)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            a = chosen_vehicle_indices\n",
    "\n",
    "            test_acc.append(float((test_y == a).sum()) / len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "\n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e + 1, sum_loss,\n",
    "                                                                                                      np.mean(acc),\n",
    "                                                                                                      test_loss,\n",
    "                                                                                                      np.mean(\n",
    "                                                                                                          test_acc)),\n",
    "          end=\"\")\n",
    "    return sum_loss, np.sum(acc) / len(acc), test_loss, np.sum(test_acc) / len(test_acc), idx_failures\n",
    "\n",
    "\n",
    "def evaluating_model(train_tables, test_tables, model, im_size, n_epochs,\n",
    "                     simple=False, weighted=0.5, lr = 0.0001, minibatch_size= 50,\n",
    "                    ):\n",
    "                    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss()\n",
    "    criterion1 = nn.MSELoss()\n",
    "\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                        epoch, im_size, weighted, minibatch_size)\n",
    "\n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2 * np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc == max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "\n",
    "    return loss, acc, test_loss, test_acc, idx_f, times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE 1. Double Input Double Output - Complete Final Net. IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 178.203 Accuracy: 0.711 Test Loss: 210.957 Accuracy: 0.036\n",
      "Average time per epoch 34.390s +- 17.257\n",
      "Max accuracy of 0.039 achieved at epoch 12\n",
      "Epoch 20. Train Loss: 182.872 Accuracy: 0.698 Test Loss: 209.130 Accuracy: 0.037\n",
      "Average time per epoch 41.804s +- 4.368\n",
      "Max accuracy of 0.037 achieved at epoch 17\n",
      "Epoch 20. Train Loss: 185.452 Accuracy: 0.698 Test Loss: 211.922 Accuracy: 0.032\n",
      "Average time per epoch 42.932s +- 4.301\n",
      "Max accuracy of 0.039 achieved at epoch 5\n",
      "Epoch 20. Train Loss: 184.251 Accuracy: 0.694 Test Loss: 210.898 Accuracy: 0.030\n",
      "Average time per epoch 43.972s +- 2.989\n",
      "Max accuracy of 0.034 achieved at epoch 1\n",
      "Average accuracy 0.037 +- 0.002. Av loss 193.689\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n",
      "\n",
      "CASE 2. Double Input Double Output - Complete Final Net. Learning rate = 0.001 IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 55.005 Accuracy: 0.874 Test Loss: 328.264 Accuracy: 0.0366\n",
      "Average time per epoch 43.064s +- 4.121\n",
      "Max accuracy of 0.038 achieved at epoch 13\n",
      "Epoch 20. Train Loss: 62.170 Accuracy: 0.855 Test Loss: 331.129 Accuracy: 0.0333\n",
      "Average time per epoch 41.343s +- 4.190\n",
      "Max accuracy of 0.040 achieved at epoch 5\n",
      "Epoch 20. Train Loss: 62.106 Accuracy: 0.852 Test Loss: 298.592 Accuracy: 0.0400\n",
      "Average time per epoch 42.536s +- 4.508\n",
      "Max accuracy of 0.042 achieved at epoch 16\n",
      "Epoch 20. Train Loss: 59.260 Accuracy: 0.862 Test Loss: 315.785 Accuracy: 0.0367\n",
      "Average time per epoch 45.782s +- 4.691\n",
      "Max accuracy of 0.040 achieved at epoch 17\n",
      "Average accuracy 0.040 +- 0.002. Av loss 192.497\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n",
      "\n",
      "CASE 3. Double Input Double Output - Reduced Final Net. IMAGE SIZE = 30\n",
      "\n",
      "Epoch 20. Train Loss: 118.595 Accuracy: 0.860 Test Loss: 247.100 Accuracy: 0.034\n",
      "Average time per epoch 36.915s +- 9.933\n",
      "Max accuracy of 0.034 achieved at epoch 16\n",
      "Epoch 20. Train Loss: 123.696 Accuracy: 0.852 Test Loss: 235.024 Accuracy: 0.037\n",
      "Average time per epoch 32.903s +- 5.210\n",
      "Max accuracy of 0.040 achieved at epoch 1\n",
      "Epoch 20. Train Loss: 129.446 Accuracy: 0.824 Test Loss: 231.891 Accuracy: 0.039\n",
      "Average time per epoch 39.427s +- 6.651\n",
      "Max accuracy of 0.047 achieved at epoch 4\n",
      "Epoch 20. Train Loss: 122.121 Accuracy: 0.847 Test Loss: 238.503 Accuracy: 0.037\n",
      "Average time per epoch 34.890s +- 8.731\n",
      "Max accuracy of 0.041 achieved at epoch 4\n",
      "Average accuracy 0.040 +- 0.005. Av loss 182.780\n",
      " -------------\n",
      "Num parameters: 435740\t Num Trainable parameters: 435740\n"
     ]
    }
   ],
   "source": [
    "print('\\nCASE 1. Double Input Double Output - Complete Final Net. IMAGE SIZE = 30\\n')\n",
    "total_acc, total_loss = [], []\n",
    "im_size = 30\n",
    "nv = 30\n",
    "kernel_size=3\n",
    "n_epochs = 20\n",
    "\n",
    "np.random.seed(4914)\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    tables = list(range(1, 90))\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model1= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model1, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "    \n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model1.parameters()), sum(\n",
    "        p.numel() for p in model1.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 2. Double Input Double Output - Complete Final Net. Learning rate = 0.001 IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model2= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model2, IMAGE_SIZE, n_epochs, lr = 0.001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model2.parameters()), sum(\n",
    "        p.numel() for p in model2.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 3. Double Input Double Output - Reduced Final Net. IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model3= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model3, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model3.parameters()), sum(\n",
    "        p.numel() for p in model3.parameters() if p.requires_grad)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_tables[0]\n",
    "solutions= solutions_tables[0]\n",
    "\n",
    "x,x_aux,y, y_aux = get_network_tensors([i for i in range(30)], FLEET_SIZE, IMAGE_SIZE, data, solutions)\n",
    "situation_data = x[2]\n",
    "sd = situation_data\n",
    "\n",
    "nnb= nearest_neighbour_policy(data[1])\n",
    "print(\"\")\n",
    "print(\"Nearest neighbour:\",nnb)\n",
    "#model= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "out_aux,out = model2(x, x_aux)\n",
    "_, out_i = torch.max(out, 1)\n",
    "print(\"Result comparison: main output\")\n",
    "print(\"Network:\",out_i)\n",
    "print(\"Policy:\",y)\n",
    "print(\"\")\n",
    "print(\"Result comparison: auxiliary output\")\n",
    "print(\"Network:\",out_aux)\n",
    "print(\"Policy:\",y_aux)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Linear model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
