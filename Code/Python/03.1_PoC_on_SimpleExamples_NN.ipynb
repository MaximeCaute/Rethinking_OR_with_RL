{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Proof of Concept on Simple Examples : Nearest neighbour\n",
    "\n",
    "The purpose of this notebook is to test our learning ability on simple, non-stochastic policies, for validation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here consider a simple policy with Nearest Neighbor Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2793e2f2ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#importing functions for time evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate_times\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/maxime/DATA/Maxime/Documents/Scolarité/EPFL/Projet/Rethinking_OR_with_RL/Code/Python/evaluate_times.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSimulatorNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVehicle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_vehicle_assignment_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_vehicle_assignment_random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/maxime/DATA/Maxime/Documents/Scolarité/EPFL/Projet/Rethinking_OR_with_RL/Code/Python/SimulatorNN.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/maxime/DATA/Maxime/Documents/Scolarité/EPFL/Projet/Rethinking_OR_with_RL/Code/Python/Utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mosmnx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/osmnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0melevation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfootprints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/osmnx/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/geopandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_points_from_xy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpoints_from_xy\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_file\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_postgis\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msjoin\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeoSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlibdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrvsupport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupported_drivers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_env_with_credentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mItemsIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeysIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuffer_to_virtual_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_virtual_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGEOMETRY_TYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/condenv/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pytorch, for NNs\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Time, for time perf. measurements\n",
    "import time\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "#importing functions for time evaluation\n",
    "from evaluate_times import generate_event, generate_times, print_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 30\n",
    "\n",
    "REQUESTS_PER_TABLE = 100\n",
    "\n",
    "FLEET_SIZE = 30\n",
    "VEHICLES_CAPACITY = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data\n",
    "\n",
    "For simplicity purpose, at first our data will be created out of thin air for the requests.\n",
    "\n",
    "We will keep a 30 \\* 30 pixels image, where requests will be asked from two random points. Vehicles will be randomly positioned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixel(image_size = 30):\n",
    "    x,y = np.random.randint(low= 0, high = image_size, size = (2,))\n",
    "    return [x,y]\n",
    "\n",
    "def generate_vehicles_info(image_size = 30, fleet_size = 30, vehicles_capacity = -1):\n",
    "    \"\"\"\n",
    "        - consider_capacity: int or int list. The capacity of the vehicles. \n",
    "            If it is an integer, the fleet is homogenous with respect to capacity, and have this value.\n",
    "            If a value is -1, the vehicle has infinite capacity\n",
    "    \"\"\"\n",
    "    vehicles_info = []\n",
    "    \n",
    "    # Setting capacities properly\n",
    "    if isinstance(vehicles_capacity, int):\n",
    "        value = vehicles_capacity\n",
    "        vehicles_capacity = [value for i in range(fleet_size)]\n",
    "    \n",
    "    \n",
    "    for k in range(fleet_size):\n",
    "        vehicle_position = random_pixel(image_size)\n",
    "        vehicles_destination = random_pixel(image_size)\n",
    "        vehicle_capacity = vehicles_capacity[k]\n",
    "        vehicle_load = np.random.randint(low = 0,high=vehicle_capacity)\n",
    "        \n",
    "        vehicle_info = [vehicle_capacity, vehicle_load]+ vehicle_position+ vehicles_destination\n",
    "        \n",
    "        vehicles_info.append(vehicle_info)\n",
    "    return vehicles_info\n",
    "\n",
    "def generate_data_table(request_amount,image_size = 30, fleet_size=30, vehicles_capacity =-1):\n",
    "    data = {}\n",
    "    for n in range(request_amount):\n",
    "\n",
    "        request = []\n",
    "\n",
    "        pickup_location = random_pixel(image_size)\n",
    "        dropoff_location = random_pixel(image_size)\n",
    "\n",
    "        request_info = pickup_location + dropoff_location\n",
    "        vehicles_info = generate_vehicles_info(image_size, fleet_size, vehicles_capacity)\n",
    "\n",
    "        request.append(request_info)\n",
    "\n",
    "        data[n] = request+vehicles_info\n",
    "    return data\n",
    "\n",
    "data_tables = []\n",
    "for i in range(90):\n",
    "    data_tables.append(generate_data_table(REQUESTS_PER_TABLE,IMAGE_SIZE, FLEET_SIZE, VEHICLES_CAPACITY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_tables[0][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy definition\n",
    "\n",
    "We now define our policy, which basically implements nearest neighbour search (without capacity constraints, at first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_policy(data):\n",
    "    request_info = data[0]\n",
    "    pickup_location = request_info[0:2]\n",
    "    \n",
    "    vehicles_request_infos = data[1:-1]\n",
    "    \n",
    "    closest_dist = 10000\n",
    "    selected_vehicle = -1\n",
    "    for i,vehicle_request_infos in enumerate(vehicles_request_infos):\n",
    "        vehicle_position = vehicle_request_infos[2:4]\n",
    "        \n",
    "        vehicle_distance = np.sqrt(\n",
    "                    (vehicle_position[0]-pickup_location[0])**2 +\n",
    "                    (vehicle_position[1]-pickup_location[1])**2 \n",
    "        )\n",
    "        \n",
    "        if vehicle_distance < closest_dist:\n",
    "            selected_vehicle = i+1\n",
    "            closest_dist = vehicle_distance\n",
    "    \n",
    "    return selected_vehicle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a much simpler policy: always calling vehicle 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_car_policy(data):\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus generate the solutions from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solutions(data, policy):\n",
    "    solutions = {}\n",
    "    for k in data.keys():\n",
    "\n",
    "        selected_vehicle = policy(data[k])\n",
    "\n",
    "        solutions[k] = selected_vehicle\n",
    "    return solutions\n",
    "\n",
    "solutions_tables = []\n",
    "for data in data_tables:\n",
    "    solutions_tables.append(generate_solutions(data, nearest_neighbour_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solutions_tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation\n",
    "\n",
    "We now define the neural network we will use for learning. Note that this is the one previously used for non-recurrent inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        # add vehicles information\n",
    "        x7 = torch.cat([v_x.transpose(2,1) ,x6.view(-1, 1,v_x.shape[1] )], dim=1).view(v_x.shape[0], -1)\n",
    "        \n",
    "        x8 = self.fc3(x7)\n",
    "        x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Final2(nn.Module):\n",
    "    def __init__(self, inp1, num_v, im_size, kernel):\n",
    "        super().__init__()\n",
    "\n",
    "        ins = 5\n",
    "        self.cs = kernel\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(inp1, ins, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(ins),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ins, 5, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(5, 2, kernel_size=self.cs),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2*(im_size - 3*(self.cs-1))**2, 264),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(264, num_v),\n",
    "        )\n",
    "        \n",
    "        self.f_aux = nn.Sequential(\n",
    "            nn.Linear(num_v, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(7*num_v, 500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(500, num_v),\n",
    "            nn.BatchNorm1d(num_v),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, v_x):\n",
    "        num_v = v_x.shape[1] \n",
    "        \n",
    "        x0 = x\n",
    "        x1 = self.conv1(x0)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        x4 = x3.view(-1, 2*(x.shape[-1] - 3*(self.cs-1))*(x.shape[-1] - 3*(self.cs-1)))\n",
    "        x5 = self.fc1(x4)\n",
    "        x6 = self.fc2(x5)\n",
    "        \n",
    "        x_aux = self.f_aux(x6)\n",
    "        #x7=x6\n",
    "        \n",
    "        #x8 = self.fc3(x7)\n",
    "        #x9 = self.fc4(x8)\n",
    "        \n",
    "        return x_aux, x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_tensors(batch_indices, vehicles_amount, image_size, \n",
    "                       data, data_y):\n",
    "    \"\"\"\n",
    "    Instantiate training data for later filling and tensor conversion.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_indices)\n",
    "    dist = 1 #/ (image_size - 1)\n",
    "    \n",
    "    main_input = np.zeros((batch_size, vehicles_amount+1, image_size, image_size))\n",
    "    main_output = []\n",
    "    # 2D image\n",
    "    aux_input = []\n",
    "    aux_output = np.zeros((batch_size, 2))\n",
    "    \n",
    "    \n",
    "    for k, cl in enumerate(batch_indices):\n",
    "        \n",
    "        # Getting input for request\n",
    "        situation_data = data[cl]\n",
    "        \n",
    "        # Filling request-related information in main input\n",
    "        request_data = situation_data[0]\n",
    "        pickup_xpos, pickup_ypos = int(situation_data[0][0] // dist), int(situation_data[0][1] // dist)\n",
    "        dropoff_xpos, dropoff_ypos = int(situation_data[0][2] // dist),  int(situation_data[0][3] // dist)\n",
    "        main_input[k][0][pickup_xpos][pickup_ypos] = 1\n",
    "        main_input[k][0][dropoff_xpos][dropoff_ypos] = -1\n",
    "        \n",
    "        # Filling vehicle positions in main input\n",
    "        for vehicle_index in range(1, 31):\n",
    "            vehicle_data = situation_data[vehicle_index]\n",
    "            vehicle_xpos = int(vehicle_data[2] // dist)\n",
    "            vehicle_ypos = int(vehicle_data[3] // dist)\n",
    "            \n",
    "            main_input[k][vehicle_index][vehicle_xpos][vehicle_ypos] = 1\n",
    "        \n",
    "        # Filling all vehicles information in auxiliary input\n",
    "        total_vehicles_data = situation_data[1:]\n",
    "        aux_input.append(torch.tensor(np.asarray(total_vehicles_data)).type(torch.FloatTensor))\n",
    "        \n",
    "        \n",
    "        # Getting output for request\n",
    "        assigned_vehicle = int(data_y[cl]) + 1\n",
    "        \n",
    "        # As no treatment is involved, main output is filled out of the loop.\n",
    "        \n",
    "        # Filling assigned vehicle position in auxiliary output\n",
    "        assigned_vehicle_xpos = situation_data[assigned_vehicle][2]#loc = int(data_y[cl])\n",
    "                                                                #loc_y[k] = [data[cl][loc + 1][2], data[cl][loc + 1][3]]\n",
    "        assigned_vehicle_ypos = situation_data[assigned_vehicle][3]\n",
    "        aux_output[k] = [assigned_vehicle_xpos, assigned_vehicle_ypos]\n",
    "\n",
    "    # Filling assigned vehicles - (almost no treatment involved)\n",
    "    output_list = [data_y[i] for i in batch_indices]\n",
    "    main_output = np.hstack(output_list)\n",
    "    \n",
    "    # Turning inputs to tensors\n",
    "    main_input_tensor = torch.tensor(main_input).type(torch.FloatTensor)\n",
    "    aux_input_tensor = torch.stack(aux_input).type(torch.FloatTensor)\n",
    "    main_output_tensor = torch.tensor(main_output).type(torch.LongTensor)\n",
    "    aux_output_tensor = torch.tensor(aux_output).type(torch.FloatTensor)\n",
    "\n",
    "    return main_input_tensor, aux_input_tensor, main_output_tensor, aux_output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                e, im_size, weighted, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Epochs for recurrent, double-input double-output not clipped nor expanded networks.\n",
    "    \"\"\"\n",
    "    sum_loss = 0\n",
    "    acc = []\n",
    "    idx_failures = []\n",
    "    dist = 1 / (im_size - 1)\n",
    "\n",
    "    # train data\n",
    "    for k, TABLE in enumerate(train_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "        \n",
    "        idx = list(data.keys())\n",
    "        nv = len(data[idx[0]]) - 1\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "            # idx of clients to analyse\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "                    \n",
    "            train_x, train_x_aux, train_y, train_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "            \n",
    "            #  set gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "\n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "            # take into account difference in order of magnitude\n",
    "            #batch_loss = 100*weighted * criterion1(output1, train_y_aux) + (1 - weighted) * criterion2(output2,\n",
    "            #                                                                                       train_y)\n",
    "            batch_loss = weighted * criterion1(aux_output, train_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                           train_y)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss = sum_loss + batch_loss.item()\n",
    "            a = chosen_vehicle_indices\n",
    "            acc.append(float((train_y == a).sum()) / len(train_y))\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "\n",
    "    # model.eval()\n",
    "    for k, TABLE in enumerate(test_tables):\n",
    "        \n",
    "        data, data_y = data_tables[TABLE], solutions_tables[TABLE]\n",
    "\n",
    "        idx = list(data.keys())\n",
    "\n",
    "        for b in range(0, len(idx), mini_batch_size):\n",
    "\n",
    "            t_idx = idx[b:b + mini_batch_size]\n",
    "\n",
    "            test_x, test_x_aux, test_y, test_y_aux = get_network_tensors(t_idx, nv, im_size, data, data_y)\n",
    "\n",
    "            main_input = train_x\n",
    "            vehicle_input = train_x_aux\n",
    "            \n",
    "            # Inputing informations\n",
    "            aux_output, main_output = model(main_input, vehicle_input)\n",
    "            _, chosen_vehicle_indices = torch.max(main_output, 1)\n",
    "            \n",
    "                \n",
    "            batch_loss = weighted * criterion1(aux_output, test_y_aux) + (1 - weighted) * criterion2(main_output,\n",
    "                                                                                                          test_y)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "            \n",
    "            a = chosen_vehicle_indices\n",
    "\n",
    "            test_acc.append(float((test_y == a).sum()) / len(test_y))\n",
    "            idx_failures += [t_idx[i] for i in np.where(test_y != a)[0]]\n",
    "\n",
    "    print('\\rEpoch {}. Train Loss: {:.3f} Accuracy: {:.3f} Test Loss: {:.3f} Accuracy: {:.3f}'.format(e + 1, sum_loss,\n",
    "                                                                                                      np.mean(acc),\n",
    "                                                                                                      test_loss,\n",
    "                                                                                                      np.mean(\n",
    "                                                                                                          test_acc)),\n",
    "          end=\"\")\n",
    "    return sum_loss, np.sum(acc) / len(acc), test_loss, np.sum(test_acc) / len(test_acc), idx_failures\n",
    "\n",
    "\n",
    "def evaluating_model(train_tables, test_tables, model, im_size, n_epochs,\n",
    "                     simple=False, weighted=0.5, lr = 0.0001, minibatch_size= 50,\n",
    "                    ):\n",
    "                    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion2 = nn.CrossEntropyLoss()\n",
    "    criterion1 = nn.MSELoss()\n",
    "\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = [], [], [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        current_t = time.time()\n",
    "        train_l, accuracy, test_l, test_a, idx_failures = \\\n",
    "            epoch_train(model, train_tables, test_tables, optimizer, criterion1, criterion2, \\\n",
    "                        epoch, im_size, weighted, minibatch_size)\n",
    "\n",
    "        times.append(time.time() - current_t)\n",
    "        loss.append(train_l)\n",
    "        test_loss.append(test_l)\n",
    "        acc.append(accuracy)\n",
    "        test_acc.append(test_a)\n",
    "        idx_f.append(idx_failures)\n",
    "\n",
    "    print('\\nAverage time per epoch {:.3f}s +- {:.3f}'.format(np.mean(times), 2 * np.std(times)))\n",
    "\n",
    "    max_acc = np.max(test_acc)\n",
    "    iter_max = np.where(test_acc == max_acc)\n",
    "\n",
    "    print('Max accuracy of {:.3f} achieved at epoch {}'.format(max_acc, iter_max[0][0]))\n",
    "\n",
    "    return loss, acc, test_loss, test_acc, idx_f, times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCASE 1. Double Input Double Output - Complete Final Net. IMAGE SIZE = 30\\n')\n",
    "total_acc, total_loss = [], []\n",
    "im_size = 30\n",
    "nv = 30\n",
    "kernel_size=3\n",
    "n_epochs = 20\n",
    "\n",
    "np.random.seed(4914)\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    tables = list(range(1, 90))\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model1= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model1, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "    \n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model1.parameters()), sum(\n",
    "        p.numel() for p in model1.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 2. Double Input Double Output - Complete Final Net. Learning rate = 0.001 IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model2= Net_Final(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model2, IMAGE_SIZE, n_epochs, lr = 0.001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model2.parameters()), sum(\n",
    "        p.numel() for p in model2.parameters() if p.requires_grad)))\n",
    "\n",
    "print('\\nCASE 3. Double Input Double Output - Reduced Final Net. IMAGE SIZE = 30\\n')\n",
    "\n",
    "total_acc, total_loss = [], []\n",
    "\n",
    "np.random.seed(4914)\n",
    "tables = list(range(1, 90))\n",
    "\n",
    "\n",
    "main_acc, main_loss = [], []\n",
    "for trial in range(4):\n",
    "    # Selecting train/ test tables for every trial\n",
    "    np.random.shuffle(tables)\n",
    "    train_tables, test_tables, validation_tables = \\\n",
    "    tables[:int(len(tables) * 0.6)], tables[int(len(tables) * 0.6):int(len(tables) * 0.9)], tables[int(\n",
    "            len(tables) * 0.9):]\n",
    "    \n",
    "    model3= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "    loss, acc, test_loss, test_acc, idx_f, times = \\\n",
    "        evaluating_model(train_tables, test_tables, model3, IMAGE_SIZE, n_epochs, lr = 0.0001, weighted=0)\n",
    "\n",
    "    main_loss.append(min(test_loss))\n",
    "    main_acc.append(max(test_acc))\n",
    "total_acc.append(main_acc)\n",
    "total_loss.append(main_loss)\n",
    "print('Average accuracy {:.3f} +- {:.3f}. Av loss {:.3f}\\n -------------'.format( \\\n",
    "            np.mean(main_acc), np.std(main_acc), np.mean(main_loss)))\n",
    "print('Num parameters: {}\\t Num Trainable parameters: {}'.format(sum(p.numel() for p in model3.parameters()), sum(\n",
    "        p.numel() for p in model3.parameters() if p.requires_grad)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_tables[0]\n",
    "solutions= solutions_tables[0]\n",
    "\n",
    "x,x_aux,y, y_aux = get_network_tensors([i for i in range(12)], FLEET_SIZE, IMAGE_SIZE, data, solutions)\n",
    "situation_data = x[0]\n",
    "sd = situation_data\n",
    "\n",
    "nnb= nearest_neighbour_policy(data[1])\n",
    "print(\"\")\n",
    "print(\"Nearest neighbour:\",nnb)\n",
    "#model= Net_Final2(FLEET_SIZE+1, FLEET_SIZE, IMAGE_SIZE, kernel_size)\n",
    "out_aux,out = model2(x, x_aux)\n",
    "_, out_i = torch.max(out, 1)\n",
    "print(\"Result comparison: main output\")\n",
    "print(\"Network:\",out_i)\n",
    "print(\"Policy:\",y)\n",
    "print(\"\")\n",
    "print(\"Result comparison: auxiliary output\")\n",
    "print(\"Network:\",out_aux)\n",
    "print(\"Policy:\",y_aux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Linear model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
